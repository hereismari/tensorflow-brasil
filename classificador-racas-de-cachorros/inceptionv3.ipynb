{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Classificador de Raças de Cachorros usando Tensorflow e Keras\n",
    "\n",
    "\n",
    "Neste notebook iremos implementadar um modelo para classificação de imagens. Classificação é uma das \"tarefas\" em que podemos utilizar Machine Learning, nesta tarefa o ensino é **supervisionado**, em outras palavras nós vamos ensinar ao modelo através de exemplos com gabarito.\n",
    "\n",
    "Nosso modelo deverá receber imagens de veículos e não-veículos e identificar a que **classe** (raça de cachorro) o cachorro pertence.\n",
    "\n",
    "## Dados\n",
    "\n",
    "Os dados são oriundos da competição [Dog Breed Indentification do Kaggle](https://www.kaggle.com/c/dog-breed-identification), na qual fornece aproximadamente 10 mil imagens de cachorros de 120 classes.\n",
    "\n",
    "\n",
    "## Modelo\n",
    "\n",
    "Iremos utilizar a [arquitetura da  InceptionV3](https://arxiv.org/abs/1512.00567), ela está implementada no [Keras](https://keras.io/applications/#inceptionv3)\n",
    "\n",
    "\n",
    "## Conseguindo os dados\n",
    "\n",
    "Para ter acesso aos dados é necessário uma conta no Kaggle, e ter que entrar na [competição](https://www.kaggle.com/c/dog-breed-identification), e ir na aba Data na competição a baixá-los\n",
    "\n",
    "### Avisos\n",
    "\n",
    "#### Aviso #1\n",
    "Para fazer o treinamento da InceptionV3 é necessário um grande poder computacional, na qual a maioria das pessoas não possuem. Mas não será por isso que não utilizaremos a Inception, graças ao Kaggle, temos a opção de rodar Kernels (que são muito similares aos notebooks do jupyter) na infraestrutura do próprio Kaggle, para mais informações sobre o suporte a GPU's do Kaggle veja [esse notebook](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu) do [Dan Becker](https://twitter.com/dan_s_becker)\n",
    "\n",
    "#### Aviso #2\n",
    "Esse notebook não foi executado na minha máquina, eu rodei ele nos kernels do Kaggle. Por isso não temos as saídas das células, se você quiser visualizar as saídas clique [aqui](https://www.kaggle.com/igorslima/inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "02c8a71ece29a178232e9dbaa7e276771b9e22c7"
   },
   "outputs": [],
   "source": [
    "input_folder = '/kaggle/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f1befac14426ddfd086dff71415ebf116cff4ba"
   },
   "outputs": [],
   "source": [
    "# lendo input\n",
    "df_train = pd.read_csv(input_folder+'/labels.csv')\n",
    "df_test = pd.read_csv(input_folder+'/sample_submission.csv')\n",
    "df_train.breed.value_counts().plot(kind='bar', figsize=(15,15), title=\"Quantidade de imagens por raça no treino\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "504b2d4ca611b4ad14590ea75265acdca9dd5791"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58b2f589d55b4f1c65e1029d733e892edde27e93"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d0e60963df94f6c46ed02468bfad4ac9678a9a1"
   },
   "source": [
    "## Transormando os dados para a \"notação\" one-hot-encoding\n",
    "\n",
    "Para mais informações sobre o One Hot Enconding leia este [post](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e3fb9189bc51fdc317fab64c191b5854aa58370"
   },
   "outputs": [],
   "source": [
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1eb0f06b0ae1b529c3dc6cbca9b7c111ed13d093"
   },
   "outputs": [],
   "source": [
    "im_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13dcdd5e58d1bbf690ad5d03b2cd10b06fc137ba"
   },
   "source": [
    "## Lendo as imagens\n",
    "\n",
    "Para treinar a rede é necessário peger as imagens do disco e colocar elas em memória. Não entendeu um 'a' do que eu disse? Tudo bem, é normal. O que eu quis dizer foi que vamos ter que pegar as imagens do HD e colocar elas na memória RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d222cdecad7acd231c1219b4a34644e0afc58d3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm # bliblioteca para colocar a porcentagem de andamento do for\n",
    "import cv2 # biblioteca para visão computacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48eea7f6f97e88dac6d5bb2f74bd944eda536b0b"
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1358b9a8fe44b3c46869a5f742a19bff831fc809"
   },
   "outputs": [],
   "source": [
    "i = 0 \n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread(input_folder+'/train/{}.jpg'.format(f))\n",
    "    x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "    label = one_hot_labels[i]\n",
    "    y_train.append(label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef7b84bca44b8b98e8e595a8c5b88ed5208708be"
   },
   "outputs": [],
   "source": [
    "del df_train # apagando uma variável pra diminuir consumo de memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2459fc750bd49ed0b58c18850354b1b702456ce2"
   },
   "outputs": [],
   "source": [
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread(input_folder+'/test/{}.jpg'.format(f))\n",
    "    x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab1ed63653409cbb17f079312858827753939815"
   },
   "source": [
    "## Dividindo dataset\n",
    "\n",
    "Geralmente em dividimos os dados em treino, validação e teste.\n",
    "1. Treino: conjunto para treinar o modelo\n",
    "2. Validação: conjunto para escolher os melhores hiperparâmetros do modelo (mais tarde falo sobre hiperparâmetros, ok?)\n",
    "3. Teste: conjunto para coletar as métricas finais do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8330926bebc8b2e681db5fc0a10ad663598ab9e7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # biblioteca para fazer a divisão dos dados em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "643ec57c09549f9034d1d1c14cb0618f6aca09d7"
   },
   "outputs": [],
   "source": [
    "num_class = 120\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, shuffle=True,  test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b89b322fe04cd8a6aae2017d1920d4e7e9d7c55"
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "Nós temos dados o suficiente para travar nossas máquinas XD, mas não o suficiente para treinar modelos bastantes robustos, temos poucas imagens por classe.\n",
    "\n",
    "Para ameninzar esse problema iremos utilizar uma técnica chamada data augmentations, ela transforma uma imagem em diversas, como por exemplo dar um giro vertical, ou horizontal. Como nesse exemplo:\n",
    "\n",
    "![Imgur](https://i.imgur.com/GJGMou5.png)\n",
    "\n",
    "Links legais (em inglês, desculpem):\n",
    "\n",
    "[Link para a documentação](https://keras.io/preprocessing/image/)\n",
    "\n",
    "[Tutorial massa do keras](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "\n",
    "[Outro tutorial massa, mas não é do Keras, esse é do Dan Becker](https://www.kaggle.com/dansbecker/data-augmentation)[](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1bfefd063c2286770a4a84a60f9c46edf754416"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator # biblioteca para data augmetantaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0511f3cdfda94276de4d5e1298fb6cb14945428"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.2, \n",
    "                            height_shift_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            rotation_range=30,\n",
    "                            vertical_flip=False,\n",
    "                            horizontal_flip=True) # aqui eu defino os parâmetros que irei \n",
    "                                                  # utilizar para gerar as imagens\n",
    "#Aqui a ideia é rotacionar, inverter ... as fotos para que aumente o tamanho do Dataset e diminua a chance \n",
    "# de overfittar o modelo, um gato invertido continua sendo um gato\n",
    "\n",
    "train_generator = datagen.flow(np.array(X_train), np.array(Y_train), \n",
    "                               batch_size=32) \n",
    "valid_generator = datagen.flow(np.array(X_valid), np.array(Y_valid), \n",
    "                               batch_size=32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12d0be41e87bb62d28813e408509e587536de72f"
   },
   "source": [
    "## Criação da Inception\n",
    "\n",
    "A partir de agora iremos criar a rede propriamente dita, iremos utilizar a arquitetura da rede Inception, e os pesos pré-treinada sobre os dados do ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a757b3461c8293ed4f2176416b9e6db37ae650a"
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras import regularizers\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a1c8dab8ee63ff004312811a1e425eecbfe1aaf"
   },
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights=\"imagenet\",include_top=False, input_shape=(im_size, im_size, 3))\n",
    "dropout = base_model.output\n",
    "dropout = Dropout(0.5)(dropout)\n",
    "model_with_dropout = Model(inputs=base_model.input, outputs=dropout)\n",
    "    \n",
    "x = model_with_dropout.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_class, activation='softmax',\n",
    "                    kernel_regularizer=regularizers.l2(0.0015),\n",
    "                    activity_regularizer=regularizers.l1(0.0015))(x)\n",
    "    \n",
    "my_model = Model(inputs=model_with_dropout.input, outputs=predictions)\n",
    "        \n",
    "my_model.compile(optimizer='sgd',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01d72b62414ca1690bff9d179c57c8375375cbc6"
   },
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cf300f822959cf2b29c860ffc0219ced4f873cc"
   },
   "outputs": [],
   "source": [
    "my_model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=10, steps_per_epoch=len(X_train) / 18,\n",
    "    validation_data=valid_generator, validation_steps=len(X_valid) / 18) # reali\n",
    "my_model.save_weights('first_try.h5')  # Criando um modelo .h5 conseguimos dar load nos parametros\n",
    "#Sem ter que ficar treinando toda vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce948f90bb140680168430850b4b910f5097f924"
   },
   "source": [
    "## Fazendo predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "489a3eacfb5d2d2980eb0772ff880cd90e195aa4"
   },
   "outputs": [],
   "source": [
    "preds = my_model.predict(np.array(x_test), verbose=1)\n",
    "sub = pd.DataFrame(preds)\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "sub.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
