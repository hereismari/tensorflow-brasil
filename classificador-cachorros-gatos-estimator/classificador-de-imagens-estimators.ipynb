{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador de Imagens usando TensorFlow Estimators\n",
    "\n",
    "Neste notebook iremos implementadar um modelo para classificação de imagens. Classificação é uma das \"tarefas\" em que podemos utilizar Machine Learning, nesta tarefa o ensino é **supervisionado**, em outras palavras nós vamos ensinar ao modelo através de exemplos com gabarito.\n",
    "\n",
    "Nosso modelo deverá receber imagens de gatos e cachorros e identificar a que **classe** (gato ou cachorro) estas imagens pertencem.\n",
    "\n",
    "## Dados\n",
    "\n",
    "Os dados foram retirados da base de dados [CIFAR 10](https://www.cs.toronto.edu/~kriz/cifar.html) que contém 10000 imagens de 10 classes distintas, para este exemplo iremos utilizar apenas as classes gato e cachorro.\n",
    "\n",
    "## Modelo\n",
    "\n",
    "Iremos utilizar diferentes modelos com diferentes níveis de complexidade.\n",
    "\n",
    "## Créditos\n",
    "\n",
    "Essa atividade é baseada no notebook encontrado [aqui](https://github.com/random-forests/tensorflow-workshop/tree/master/extras/cat_dog_estimator) implementada originalmente por [@chrisying](https://github.com/chrisying).\n",
    "\n",
    "Além disso para mais informações sobre Estimators [este](goo.gl/DBeUkN) é um ótimo material para mais detalhes!\n",
    "\n",
    "Obrigada a todos os envolvidos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sua versão do TensorFlow: 1.4.1\n",
      "Recomenda-se para esta atividade uma versão >= 1.4.0\n"
     ]
    }
   ],
   "source": [
    "# Compatibilidade entre Python 2 e Python 3\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.INFO)  # Permitindo visualização de logs\n",
    "\n",
    "# Bibliotecas auxiliares\n",
    "import _pickle as cPickle  # maior eficiência ao processar as imagens\n",
    "import numpy as np  # manipular vetores\n",
    "from PIL import Image  # lidar com imagens\n",
    "import matplotlib.pyplot as plt  # plotar imagens\n",
    "%matplotlib inline\n",
    "\n",
    "# IMPORTANTE: essa linha garante que os números gerados aleatoriamente são previsíveis\n",
    "np.random.seed(0)\n",
    "\n",
    "print ('Sua versão do TensorFlow:', tf.__version__)\n",
    "print ('Recomenda-se para esta atividade uma versão >= 1.4.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando os Dados\n",
    "\n",
    "### Baixa e estraia os dados\n",
    "\n",
    "Para baixar os dados, execute os seguintes comandos na pasta em que se encontra este notebook.\n",
    "\n",
    "```bash\n",
    "curl -O http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "tar xvf cifar-10-python.tar.gz\n",
    "```\n",
    "\n",
    "Ou você pode baixar os dados manualmente [clicando neste link](http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) e extraí-los nesta pasta.\n",
    "\n",
    "Após extrair as pastas você deverá ver as seguintes subpastas:\n",
    "\n",
    "```bash\n",
    "cifar-10-batches-py/\n",
    "cifar-10-batches-py/data_batch_4\n",
    "cifar-10-batches-py/readme.html\n",
    "cifar-10-batches-py/test_batch\n",
    "cifar-10-batches-py/data_batch_3\n",
    "cifar-10-batches-py/batches.meta\n",
    "cifar-10-batches-py/data_batch_2\n",
    "cifar-10-batches-py/data_batch_5\n",
    "cifar-10-batches-py/data_batch_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando as imagens no formato .npy\n",
    "\n",
    "Para facilitar a manipulação das imagens iremos salvá-las como vetores utilizando a [biblioteca Numpy](http://www.numpy.org/) que é muito fácil de se trabalhar além de ser altamente compatível com o TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVOS_TREINO = ['cifar-10-batches-py/data_batch_%d' % i for i in range(1,6)]\n",
    "ARQUIVOS_TESTE = ['cifar-10-batches-py/test_batch']\n",
    "\n",
    "NUM_DADOS_TREINO = 10000\n",
    "NUM_DADOS_TESTE = 2000\n",
    "\n",
    "LABEL_INPUT_GATO = 3\n",
    "LABEL_INPUT_CACHORRO = 5\n",
    "\n",
    "LABEL_OUTPUT_GATO = 1\n",
    "LABEL_OUTPUT_CACHORRO = 0\n",
    "\n",
    "def unpickle(file):\n",
    "    '''Essa função apenas torna mais eficiente a manipulação dos dados, não se preocupe em entendê-la a fundo.'''\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "        return dict\n",
    "    \n",
    "def converter_para_numpy(arquivos, num_dados, saida_numpy):\n",
    "    '''Essa função converte os arquivos de treino e teste em arquivos .npy'''\n",
    "    \n",
    "    # Recupera cada dado (imagem, label) nos arquivos\n",
    "    dados = [unpickle(a) for a in arquivos]\n",
    "\n",
    "    # Salva as imagens e labels em arrays numpy\n",
    "    imagens = np.empty((num_dados, 32, 32, 3), dtype=np.uint8)\n",
    "    labels = np.empty((num_dados), dtype=np.uint8)\n",
    "\n",
    "    index = 0\n",
    "    for d in dados:\n",
    "        for batch_index, label in enumerate(d['labels']):\n",
    "            # Caso a imagem seja classificada como gato ou cachorro a consideramos\n",
    "            if label == LABEL_INPUT_GATO or label == LABEL_INPUT_CACHORRO:\n",
    "                # Os dados originais são armazenados no formato 1 x 3072 , convertemos para 32 x 32 x 3\n",
    "                imagens[index, :, :, :] = np.transpose(\n",
    "                  np.reshape(d['data'][batch_index, :],\n",
    "                  newshape=(3, 32, 32)),\n",
    "                  axes=(1, 2, 0))\n",
    "                # Salvamos o label de saída correto\n",
    "                if label == LABEL_INPUT_GATO:\n",
    "                    labels[index] = LABEL_OUTPUT_GATO\n",
    "                else:\n",
    "                    labels[index] = LABEL_OUTPUT_CACHORRO\n",
    "                index += 1\n",
    "\n",
    "    # Salvamos no arquivo de saída\n",
    "    np.save(saida_numpy, {'imagens': imagens, 'labels': labels})\n",
    "\n",
    "# Convertendo os arquivos de treino\n",
    "converter_para_numpy(ARQUIVOS_TREINO, NUM_DADOS_TREINO, 'treino.npy')\n",
    "# Convertendo os arquivos de teste\n",
    "converter_para_numpy(ARQUIVOS_TESTE, NUM_DADOS_TESTE, 'teste.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando se dados foram salvos corretamente\n",
    "\n",
    "Após manipular ou modificar os dados é sempre importante garantir que os dados estão no formato esperado e não foram corrompidos ou alterados indevidamente. Para isto vamos escolher algumas imagens do conjunto de treino aleatoriamente e verificá-las.\n",
    "\n",
    "> IMPORTANTE: para modelos reais é importante garantir a qualidade e integridade dos dados com maior rigor já que é fundamental a \"saúde\" dos dados para se obter um bom modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados de treino \n",
    "dados_treino = np.load('treino.npy').item()\n",
    "\n",
    "# Carregando os dados de teste \n",
    "# NÃO DEVEMOS OLHAR/MEXER NOS DADOS DE TESTE. SÓ UTILIZAREMOS ESTES DADOS PARA VALIDAR NOSSO\n",
    "# MODELO DEPOIS DO TREINO.\n",
    "dados_teste = np.load('teste.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato das imagens de treino: (10000, 32, 32, 3)\n",
      "Número de labels de treino: 10000\n",
      "--------------------------------------------------\n",
      "Formato das imagens de teste: (2000, 32, 32, 3)\n",
      "Número de labels de teste: 2000\n"
     ]
    }
   ],
   "source": [
    "print ('Formato das imagens de treino:', dados_treino['imagens'].shape)\n",
    "print ('Número de labels de treino:', dados_treino['labels'].shape[0])\n",
    "print ('-' * 50)\n",
    "print ('Formato das imagens de teste:', dados_teste['imagens'].shape)\n",
    "print ('Número de labels de teste:', dados_teste['labels'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplos de 5 imagens da base de treino\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABpCAYAAAAqXNiiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02a885b490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualizar_dados_treino(num_dados):\n",
    "    '''Essa função apresenta alguns dados de treino (imagens, labels) escolhidos aleatoriamente.\n",
    "       Parâmetros:\n",
    "           num_dados (int): número de dados que serão apresentados \n",
    "    '''\n",
    "\n",
    "    print('Exemplos de %d imagens da base de treino' % num_dados)\n",
    "    \n",
    "    # Escolhemos índices aleatórios\n",
    "    random_indices = np.random.randint(0, dados_treino['imagens'].shape[0], num_dados)\n",
    "    \n",
    "    # Buscando imagens e labels\n",
    "    imagens = dados_treino['imagens'][random_indices]\n",
    "    labels = dados_treino['labels'][random_indices]\n",
    "    \n",
    "    # Plottando imagens\n",
    "    for index, (img, label) in enumerate(zip(imagens, labels)):\n",
    "        plt.subplot(2, num_dados, index + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        plt.title('%i' % label)\n",
    "    plt.show()\n",
    "\n",
    "visualizar_dados_treino(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiper-parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints e logs serão salvos nessa paste, se quiser treinar do zero\n",
    "# delete a pasta ou tudo que tem nela.\n",
    "MODEL_DIR = 'models'\n",
    "\n",
    "# Hiper parâmetros\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "TRAIN_EPOCHS = 10\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "  \"\"\"Defines the CNN model that runs on the data.\n",
    "\n",
    "  The model we run is 3 convolutional layers followed by 1 fully connected\n",
    "  layer before the output. This is much simpler than most CNN models and is\n",
    "  designed to run decently on CPU. With a GPU, it is possible to scale to\n",
    "  more layers and more filters per layer.\n",
    "\n",
    "  Args:\n",
    "      features: batch_size x 32 x 32 x 3 uint8 images\n",
    "      labels: batch_size x 1 uint8 labels (0 or 1)\n",
    "      mode: TRAIN, EVAL, or PREDICT\n",
    "\n",
    "  Returns:\n",
    "      EstimatorSpec which defines the model to run\n",
    "  \"\"\"\n",
    "\n",
    "  # Preprocessando as features para o intervalo [-0.5, 0.5]\n",
    "  features = tf.cast(features, tf.float32)\n",
    "  features = (features / 255.0) - 1.0\n",
    "\n",
    "  # Define the CNN network\n",
    "  # conv1: 32 x 32 x 3 -> 32 x 32 x 16\n",
    "  conv1 = tf.layers.conv2d(\n",
    "      inputs=features,\n",
    "      filters=16,                 # 16 channels after conv\n",
    "      kernel_size=3,              # 3x3 conv kernel\n",
    "      padding='same',             # Output tensor is same shape\n",
    "      activation=tf.nn.relu)      # ReLU activation\n",
    "\n",
    "  # pool1: 32 x 32 x 16 -> 16 x 16 x 16\n",
    "  pool1 = tf.layers.max_pooling2d(\n",
    "      inputs=conv1,\n",
    "      pool_size=2,\n",
    "      strides=2)                  # Downsample 2x\n",
    "\n",
    "  # conv2: 16 x 16 x 16 -> 16 x 16 x 32\n",
    "  conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=32,\n",
    "      kernel_size=3,\n",
    "      padding='same',\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # pool2: 16 x 16 x 32 -> 8 x 8 x 32\n",
    "  pool2 = tf.layers.max_pooling2d(\n",
    "      inputs=conv2,\n",
    "      pool_size=2,\n",
    "      strides=2)\n",
    "\n",
    "  # conv3: 8 x 8 x 32 -> 8 x 8 x 64\n",
    "  conv3 = tf.layers.conv2d(\n",
    "      inputs=pool2,\n",
    "      filters=64,\n",
    "      kernel_size=3,\n",
    "      padding='same',\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # flat: 8 x 8 x 64 -> 4096\n",
    "  flat = tf.contrib.layers.flatten(conv3)\n",
    "\n",
    "  # dense: 4096 -> 1000\n",
    "  dense = tf.layers.dense(\n",
    "      inputs=flat,\n",
    "      units=1000,\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "  # saida: 1000 -> 2\n",
    "  saida = tf.layers.dense(\n",
    "      inputs=dense,\n",
    "      units=2)\n",
    "\n",
    "  # Softmax -> gera valores similares a probabilidades\n",
    "  probs = tf.nn.softmax(saida)\n",
    "\n",
    "  predicao = tf.argmax(\n",
    "      input=probs,\n",
    "      axis=1,\n",
    "      output_type=tf.int32)\n",
    "\n",
    "  # Return maximum prediction if we're running PREDICT\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions={\n",
    "            'predicao': predicao,\n",
    "            'probabilidade': probs})\n",
    "\n",
    "  # Função \"loss\" e optimizer\n",
    "  loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=tf.one_hot(labels, depth=2),\n",
    "      logits=saida)\n",
    "\n",
    "  op_treino = tf.train.MomentumOptimizer(\n",
    "      LEARNING_RATE, MOMENTUM).minimize(\n",
    "          loss=loss,\n",
    "          global_step=tf.train.get_global_step())\n",
    "  \n",
    "  # Métricas para avaliar modelo\n",
    "  metricas_eval = {\n",
    "      'acuracia': tf.metrics.accuracy(\n",
    "          labels=labels,\n",
    "          predictions=predicao)}\n",
    "\n",
    "  # EVAL uses loss and eval_metric_ops, TRAIN uses loss and train_op\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      loss=loss,\n",
    "      train_op=op_treino,\n",
    "      eval_metric_ops=metricas_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_funcao_entrada(treino, dados):\n",
    "    \"\"\"Wrapper para função de entrada de treino e teste.\n",
    "\n",
    "       Vamos utilizar um wrapper para implementar comportamentos ligeiramente\n",
    "       distintos para treino (iremos randomizar a ordem dos dados e dar um \"loop\" nos dados)\n",
    "       e avaliação (não iremos mudar a ordem dos dados).\n",
    "\n",
    "       Args:\n",
    "           treino: bool for if the model is training\n",
    "\n",
    "       Returns:\n",
    "           função de entrada com a seguinte assinatura: () -> features, labels\n",
    "    \"\"\"\n",
    "    def funcao_entrada():\n",
    "        \"\"\"Iremos usar uma função de entrada já implementada para utilizar um array numpy.\"\"\"\n",
    "        funcao_np = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={'x': dados['imagens']},\n",
    "            y=dados['labels'],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=treino,  # se for treino -> randomiza a ordem dos dados\n",
    "            num_epochs=None if treino else 1)  # se for treino -> loop nos dados\n",
    "\n",
    "        features_dict, labels = funcao_np()\n",
    "        # Since the only feature is the image itself, return the image directly\n",
    "        # instead of the features dict\n",
    "        return features_dict['x'], labels\n",
    "\n",
    "    return funcao_entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f02a87a4dd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'models', '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create the estimator object that is used by train, evaluate, and predict\n",
    "# Note that model_fn is not called until the first usage of the model.\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=tf.estimator.RunConfig().replace(model_dir=MODEL_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd64c3678d0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': 'models', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.696027, step = 1\n",
      "INFO:tensorflow:global_step/sec: 13.1656\n",
      "INFO:tensorflow:loss = 0.683823, step = 101 (7.597 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2627\n",
      "INFO:tensorflow:loss = 0.705159, step = 201 (7.540 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2985\n",
      "INFO:tensorflow:loss = 0.722643, step = 301 (7.520 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 313 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.612287.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:38:07\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-313\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:38:09\n",
      "INFO:tensorflow:Saving dict for global step 313: acuracia = 0.6345, global_step = 313, loss = 0.642581\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-313\n",
      "INFO:tensorflow:Saving checkpoints for 314 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.681832, step = 314\n",
      "INFO:tensorflow:global_step/sec: 13.3107\n",
      "INFO:tensorflow:loss = 0.708408, step = 414 (7.514 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9669\n",
      "INFO:tensorflow:loss = 0.588727, step = 514 (7.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2331\n",
      "INFO:tensorflow:loss = 0.723568, step = 614 (7.557 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 626 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.630653.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:38:34\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-626\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:38:36\n",
      "INFO:tensorflow:Saving dict for global step 626: acuracia = 0.638, global_step = 626, loss = 0.63279\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-626\n",
      "INFO:tensorflow:Saving checkpoints for 627 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.621015, step = 627\n",
      "INFO:tensorflow:global_step/sec: 13.3014\n",
      "INFO:tensorflow:loss = 0.594309, step = 727 (7.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3738\n",
      "INFO:tensorflow:loss = 0.474688, step = 827 (7.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3711\n",
      "INFO:tensorflow:loss = 0.715144, step = 927 (7.479 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 939 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.581265.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:39:01\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-939\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:39:03\n",
      "INFO:tensorflow:Saving dict for global step 939: acuracia = 0.672, global_step = 939, loss = 0.580615\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-939\n",
      "INFO:tensorflow:Saving checkpoints for 940 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.642594, step = 940\n",
      "INFO:tensorflow:global_step/sec: 13.2909\n",
      "INFO:tensorflow:loss = 0.572531, step = 1040 (7.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2292\n",
      "INFO:tensorflow:loss = 0.515112, step = 1140 (7.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4053\n",
      "INFO:tensorflow:loss = 0.49585, step = 1240 (7.460 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1252 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.567459.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:39:28\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1252\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:39:29\n",
      "INFO:tensorflow:Saving dict for global step 1252: acuracia = 0.712, global_step = 1252, loss = 0.547397\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1252\n",
      "INFO:tensorflow:Saving checkpoints for 1253 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.548843, step = 1253\n",
      "INFO:tensorflow:global_step/sec: 13.3491\n",
      "INFO:tensorflow:loss = 0.533981, step = 1353 (7.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3513\n",
      "INFO:tensorflow:loss = 0.609781, step = 1453 (7.491 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3933\n",
      "INFO:tensorflow:loss = 0.591935, step = 1553 (7.466 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1565 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.44388.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:39:54\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1565\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:39:56\n",
      "INFO:tensorflow:Saving dict for global step 1565: acuracia = 0.696, global_step = 1565, loss = 0.558855\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1565\n",
      "INFO:tensorflow:Saving checkpoints for 1566 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.511261, step = 1566\n",
      "INFO:tensorflow:global_step/sec: 13.3299\n",
      "INFO:tensorflow:loss = 0.470918, step = 1666 (7.503 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3199\n",
      "INFO:tensorflow:loss = 0.530102, step = 1766 (7.508 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3481\n",
      "INFO:tensorflow:loss = 0.56302, step = 1866 (7.492 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1878 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.503438.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:40:21\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1878\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:40:23\n",
      "INFO:tensorflow:Saving dict for global step 1878: acuracia = 0.7175, global_step = 1878, loss = 0.539634\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-1878\n",
      "INFO:tensorflow:Saving checkpoints for 1879 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.343491, step = 1879\n",
      "INFO:tensorflow:global_step/sec: 13.3707\n",
      "INFO:tensorflow:loss = 0.489746, step = 1979 (7.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3959\n",
      "INFO:tensorflow:loss = 0.627411, step = 2079 (7.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3949\n",
      "INFO:tensorflow:loss = 0.4985, step = 2179 (7.466 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2191 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.61613.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:40:48\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-2191\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:40:50\n",
      "INFO:tensorflow:Saving dict for global step 2191: acuracia = 0.7085, global_step = 2191, loss = 0.566288\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-2191\n",
      "INFO:tensorflow:Saving checkpoints for 2192 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.529292, step = 2192\n",
      "INFO:tensorflow:global_step/sec: 12.0777\n",
      "INFO:tensorflow:loss = 0.520797, step = 2292 (8.281 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.1446\n",
      "INFO:tensorflow:loss = 0.479483, step = 2392 (8.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.1014\n",
      "INFO:tensorflow:loss = 0.39873, step = 2492 (7.633 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2504 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.51553.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:41:17\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-2504\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:41:19\n",
      "INFO:tensorflow:Saving dict for global step 2504: acuracia = 0.7395, global_step = 2504, loss = 0.535005\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-2504\n",
      "INFO:tensorflow:Saving checkpoints for 2505 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.38671, step = 2505\n",
      "INFO:tensorflow:global_step/sec: 13.4145\n",
      "INFO:tensorflow:loss = 0.264358, step = 2605 (7.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4147\n",
      "INFO:tensorflow:loss = 0.434158, step = 2705 (7.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.635211, step = 2805 (7.503 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2817 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.40515.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:41:44\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-2817\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:41:46\n",
      "INFO:tensorflow:Saving dict for global step 2817: acuracia = 0.729, global_step = 2817, loss = 0.548405\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-2817\n",
      "INFO:tensorflow:Saving checkpoints for 2818 into models/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.331935, step = 2818\n",
      "INFO:tensorflow:global_step/sec: 13.2863\n",
      "INFO:tensorflow:loss = 0.313908, step = 2918 (7.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3821\n",
      "INFO:tensorflow:loss = 0.478253, step = 3018 (7.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3699\n",
      "INFO:tensorflow:loss = 0.344709, step = 3118 (7.481 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3130 into models/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.409952.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-03-20:42:10\n",
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-3130\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-03-20:42:12\n",
      "INFO:tensorflow:Saving dict for global step 3130: acuracia = 0.741, global_step = 3130, loss = 0.592509\n"
     ]
    }
   ],
   "source": [
    "steps_por_epoch = dados_treino['imagens'].shape[0] / BATCH_SIZE\n",
    "\n",
    "for epoch in xrange(TRAIN_EPOCHS):\n",
    "    estimator.train(input_fn=input_fn_wrapper(True, dados_treino),\n",
    "                  steps=steps_por_epoch)\n",
    "    # Evaluating on the same dataset as training for simplicity, normally\n",
    "    # this is a very bad idea since you are not testing how well your\n",
    "    # model generalizes to unseen data.\n",
    "    estimator.evaluate(input_fn=input_fn_wrapper(False, dados_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_imagem(path_imagem):\n",
    "    \"\"\"Essa função converte uma imagem PIL para o formato esperado pelo modelo.\n",
    "    \n",
    "       Operações:\n",
    "          - Carrega a imagem\n",
    "          - Da um \"crop\" na imagem, mantendo apenas o centro da imagem.\n",
    "          - Muda o tamanho da imagem para 32 x 32\n",
    "          - Converte a imagem para o formato numpy\n",
    "\n",
    "       Parâmetros:\n",
    "           path_imagem (str): path da imagem. Imagem deve ser RGB com pelo menos 32 x 32 píxels.\n",
    "\n",
    "       Retorna:\n",
    "           numpy.array com formato [1, 32, 32, 3]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Carregando a imagem\n",
    "    img = Image.open(path_imagem)\n",
    "\n",
    "    # Calculando como cortar a imagem\n",
    "    largura, altura = img.size\n",
    "\n",
    "    min_dim = min(largura, altura)\n",
    "    esquerda = (largura - min_dim) / 2\n",
    "    topo = (altura - min_dim) / 2\n",
    "    direita = (largura + min_dim) / 2\n",
    "    baixo = (altura + min_dim) / 2\n",
    "\n",
    "    # Cortando imagem e mantendo apenas o centro\n",
    "    img = img.crop((esquerda, topo, baixo, direita))\n",
    "    \n",
    "    # Mudando o tamanho da imagem para 32 x 32\n",
    "    img = img.resize((32, 32), resample=Image.BILINEAR)\n",
    "    \n",
    "    # Convertando a imagem para o formato numpy\n",
    "    img = np.asarray(img, dtype=np.uint8)\n",
    "    img = np.reshape(img, [1, 32, 32, 3])\n",
    "\n",
    "    return img\n",
    "\n",
    "def predicao_wrapper(img):\n",
    "    '''Wrapper para função de entrada de predição.'''\n",
    "    def funcao_entrada_predicao():\n",
    "        funcao_np = tf.estimator.inputs.numpy_input_fn(\n",
    "          x={'x': img},\n",
    "          num_epochs=1,\n",
    "          shuffle=False)\n",
    "        features_dict = funcao_np()\n",
    "        return features_dict['x']\n",
    "    return funcao_entrada_predicao\n",
    "\n",
    "def plot_imagem(img):\n",
    "    '''Plota a imagem.'''\n",
    "    img = np.reshape(img, (32, 32, 3))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "def get_predicao(path_imagem):\n",
    "    '''Formata imagem corretamente e busca predição.'''\n",
    "\n",
    "    # Formata a imagem corretamente\n",
    "    img = processar_imagem(path_imagem)\n",
    "\n",
    "    # Plota imagem\n",
    "    plot_imagem(img)\n",
    "\n",
    "    # Busca predição\n",
    "    pred_dict = estimator.predict(\n",
    "        input_fn=predicao_wrapper(img)).next()\n",
    "\n",
    "    print ('Probabilidade de ser gato: %.5f\\tProbabilidade de ser cachorro: %.5f' % (pred_dict['probabilidade'][1], pred_dict['probabilidade'][0]))\n",
    "    print ('Predição %s' % ('GATO' if pred_dict['predicao'] == 1 else 'CACHORRO'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlH+H/o54+sIo4aYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02a7177650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model.ckpt-3130\n",
      "Probabilidade de ser gato: 0.86999\tProbabilidade de ser cachorro: 0.13001\n",
      "Predição GATO\n"
     ]
    }
   ],
   "source": [
    "get_predicao('imgs/gato3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Próximos passos\n",
    "\n",
    "Há várias formas de melhorar a acurácia do modelo a mais óbiva é \"tunar\" o modelo, isto é, mudar os hiper-parâmetros do nosso modelo buscando sempre obter um modelo que generaliza melhor. Exemplos de hiper-parâmetros são:\n",
    "\n",
    "* Mudar o algoritmo de otimização, estamos usando Adam que é bem popular e conhecido mas é interessante tentar outros algoritmos.\n",
    "* Mudar o tamanho do passo do algoritmo de otimização.\n",
    "* Arquitetura da CNN - adicionar mais camadas, retirar camadas, mudar o número de nós em cada camada, etc.\n",
    "* Adicionar dropout para evitar overfitting (isto é, que a CNN seja muito boa nos dados de treino, mas não generalize)\n",
    "* etc...\n",
    "\n",
    "Poderíamos também utilizar uma técnica chamada [transfer learning](http://cs231n.github.io/transfer-learning/) que é bem popular para classificação de imagem.\n",
    "\n",
    "Para mais sobre estimators, e instrunções mais detalhadas [esse é um ótimo material](goo.gl/DBeUkN) para mais detalhes :)!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
